{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4caf7cbb-06b7-4486-b352-10d0322b3c58",
   "metadata": {
    "id": "4caf7cbb-06b7-4486-b352-10d0322b3c58"
   },
   "source": [
    "### MLBD Assignment (Group 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4c5075-3158-49b9-8306-50c9ef4e6467",
   "metadata": {},
   "source": [
    "## Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "H-_TigUbcuDj",
   "metadata": {
    "id": "H-_TigUbcuDj"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade pyspark\n",
    "# !pip install --upgrade matplotlib==3.6.1\n",
    "# !pip install sklearn numpy pandas datasist\n",
    "# !pip install pyspark spark spark-nlp\n",
    "# !pip install geopandas shapely\n",
    "# !pip install matplotlib\n",
    "# !pip install plotly kaleido\n",
    "# !pip install -U kaleido\n",
    "# !pip install folium\n",
    "# !pip install geopy\n",
    "# !pip install branca\n",
    "# !pip install altair\n",
    "# !pip install pycountry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18578900-df6a-4863-a627-0fcbc87243bd",
   "metadata": {},
   "source": [
    "## Code Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "180cb9ff-2fd9-4aa1-9f30-d240db81c544",
   "metadata": {
    "id": "180cb9ff-2fd9-4aa1-9f30-d240db81c544",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles, SparkContext, SparkConf\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import DataFrame, array_contains, array_position, regexp_replace, col, when, lit, concat, array, round, ceil\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from functools import reduce  # For Python 3.x\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import glob\n",
    "import folium\n",
    "import warnings\n",
    "import geopy\n",
    "import branca\n",
    "import altair as alt\n",
    "import pycountry\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c3129e2d-0061-4d3d-bd3a-aadd6226b54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "#spark driver configuration\n",
    "conf_spark = SparkConf().set(\"spark.driver.host\", \"127.0.0.1\")\n",
    "#generate a spark content\n",
    "sc = SparkContext(conf=conf_spark)\n",
    "#create a spark session \n",
    "spark = SparkSession.builder.appName('group20').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "J_gUMJ1wKEr2",
   "metadata": {
    "id": "J_gUMJ1wKEr2"
   },
   "outputs": [],
   "source": [
    "# uncomment this line to terminate the current spark context\n",
    "# spark._sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "81035a1b-d629-4bd8-8d2d-d5c264934a01",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81035a1b-d629-4bd8-8d2d-d5c264934a01",
    "outputId": "4d6f58fd-3360-4e7d-bcfa-fe4d0d76be9a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLBD:\n",
    "    def extract_computation_dates(self):\n",
    "        \"\"\"\n",
    "        This functions extracts all the dates on the files in dataset directory\n",
    "            Returns:\n",
    "            array: list of dates in the dataset folder\n",
    "        \"\"\"\n",
    "        #read all the datasets\n",
    "        json_dates = glob.glob(\"datasets/*.json\")\n",
    "        #sort them by the name\n",
    "        json_dates.sort()\n",
    "        #extract dates from the list\n",
    "        json_dates = [json.replace('datasets/data.24h-', '').replace('.json', '') for json in json_dates]\n",
    "        \n",
    "        return json_dates\n",
    "\n",
    "    def read_all_jsons(self):\n",
    "        \"\"\"\n",
    "        This functions reads all the datasets downloaded into datasets folder\n",
    "            Returns:\n",
    "                base_data_df: list of dataframes for each day\n",
    "        \"\"\"\n",
    "        # add all files in datasets directory to spark\n",
    "        spark.sparkContext.addFile('datasets', recursive=True)\n",
    "        #get all the data in the dataset directory as json\n",
    "        jsons = glob.glob(\"datasets/*.json\")\n",
    "        #order them by name\n",
    "        jsons.sort()\n",
    "        \n",
    "        base_data_df = []\n",
    "\n",
    "        for i in range(0, len(jsons)):\n",
    "            json_obj = jsons[i]\n",
    "            #add the file to spark for processing\n",
    "            spark.sparkContext.addFile(json_obj)\n",
    "            # print(json_obj)\n",
    "            #read the json into dataframe\n",
    "            t_df = spark.read.json(\"file://\"+SparkFiles.get(json_obj))\n",
    "            #sort by timestamp\n",
    "            t_df = t_df.sort(t_df.timestamp)\n",
    "            #add the dataframe to the array\n",
    "            base_data_df.append(t_df)\n",
    "        return base_data_df\n",
    "\n",
    "    def extractSensorValues(self, base_data_df):\n",
    "        \"\"\"\n",
    "        This function extracts the data in the sensor field\n",
    "            Arguments:\n",
    "                base_data_df: dataframe\n",
    "            Returns:\n",
    "                sensor_df: dataframe\n",
    "        \"\"\"\n",
    "        #extract sensor data values, id and types\n",
    "        sensor_df = base_data_df\\\n",
    "        .withColumn('sensorId', base_data_df.sensor.getField('id').cast(StringType()))\\\n",
    "        .withColumn('sdvID', base_data_df.sensordatavalues.getField('id').cast(StringType()))\\\n",
    "        .withColumn('sdvValue', base_data_df.sensordatavalues.getField('value'))\\\n",
    "        .withColumn('sdvValue_type', base_data_df.sensordatavalues.getField('value_type'))\n",
    "        #drop unused columns after extraction\n",
    "        sensor_df = sensor_df.drop('sampling_rate', 'sensor', 'sensordatavalues')\n",
    "        \n",
    "        #filter only records with P1 and P2\n",
    "        sensor_df = sensor_df.filter(array_contains(sensor_df['sdvValue_type'], 'P1') & array_contains(sensor_df['sdvValue_type'], 'P2'))\n",
    "        #extract the position of P1 and P2 in the array\n",
    "        sensor_df = sensor_df.withColumn('P1_Pos', array_position(sensor_df.sdvValue_type, \"P1\")-1).withColumn('P2_Pos', array_position(sensor_df.sdvValue_type, \"P2\")-1)\n",
    "        #extract the value of the P1 and P2 in the array using the array position. Then convert to integer datatype from string\n",
    "        sensor_df = sensor_df.withColumn('P1', sensor_df['sdvValue'][sensor_df.P1_Pos.cast(IntegerType())])\\\n",
    "        .withColumn('P2', sensor_df['sdvValue'][sensor_df.P2_Pos.cast(IntegerType())])\n",
    "        #extract and convert P1 and P2 data to Float\n",
    "        sensor_df.withColumn('P1', sensor_df.P1.cast(FloatType())).withColumn('P2', sensor_df.P2.cast(FloatType()))\n",
    "        \n",
    "        #drop the unused columns\n",
    "        sensor_df = sensor_df.drop('sdvID','sdvValue_type', 'sdvValue', 'P1_Pos', 'P2_Pos')\n",
    "        \n",
    "        return sensor_df\n",
    "    \n",
    "    def extractLocationInfo(self, sensor_df):\n",
    "        \"\"\"\n",
    "        This function extracts the data in the location field\n",
    "            Arguments: \n",
    "                sensor_df: sensor dataframe with unprocessed location information\n",
    "            Returns:\n",
    "                location_df: location dataframe with processed location\n",
    "        \"\"\"\n",
    "        #extract the country information with latitude and longitude\n",
    "        location_df = sensor_df.withColumn('country', sensor_df.location.getField('country'))\n",
    "        location_df = location_df.withColumn('latitude', location_df.location.getField('latitude').cast(FloatType()))\\\n",
    "        .withColumn('longitude', location_df.location.getField('longitude').cast(FloatType())).drop('location')\n",
    "        \n",
    "        #UK is not a valid ISO2 code, replace with GB\n",
    "        location_df = location_df.withColumn('country', regexp_replace('country', 'UK', 'GB'))\n",
    "        \n",
    "        return location_df\n",
    "    \n",
    "    def clean_data(self, base_data_df):\n",
    "        \"\"\"\n",
    "        This function extracts the values in the dataset field\n",
    "            Arguments:\n",
    "                base_data_df: raw data with unprocessed sensor and location data\n",
    "            Returns:\n",
    "                clean_data: cleaned dataframe\n",
    "        \"\"\"\n",
    "        #convert timestamp datatype from string to timestamp type\n",
    "        base_data_df = base_data_df.withColumn('timestamp', base_data_df.timestamp.cast(TimestampType())).distinct()\n",
    "        \n",
    "        #extract sensor information\n",
    "        clean_data = self.extractSensorValues(base_data_df)\n",
    "        \n",
    "        #exract location information\n",
    "        clean_data = self.extractLocationInfo(clean_data)\n",
    "\n",
    "        #remove duplicate data as a result of expansion\n",
    "        clean_data = clean_data.distinct()\n",
    "        \n",
    "        return clean_data\n",
    "\n",
    "    def compute_country_names(self, country_df):\n",
    "        \"\"\"\n",
    "        This function extracts the values in the sensor field\n",
    "            Arguments:\n",
    "                country_df: country dataframe\n",
    "            Returns:\n",
    "                country_df: country dataframe with country name column\n",
    "                \n",
    "        \"\"\"\n",
    "        country_names = []\n",
    "        # get the full name of the each of the country codes in the dataFrame['country'] column. UK is not valid, so replace with GB\n",
    "        [country_names.append([list(country)[0], (pycountry.countries.get(alpha_2=(list(country)[0]))).name]) for country in country_df[['country']].collect()]\n",
    "        #create a dataframe from the list\n",
    "        country_df = pd.DataFrame(np.array(country_names), columns=['country', 'country_name'])\n",
    "        country_df = spark.createDataFrame(country_df)\n",
    "\n",
    "        return country_df\n",
    "\n",
    "    def createVectorTransform(self, base_data_df):\n",
    "        \"\"\"\n",
    "        This function creates a vector transform required for creating, testing and computing the accuracy of a model\n",
    "            Arguments:\n",
    "                base_data_df: base dataset with all the required data\n",
    "            Returns:\n",
    "                vecTransform: a feature transforemd representation of the input data that merges multiple columns into a vector column\n",
    "        \"\"\"\n",
    "        #create a vector assembler with latitude and longitude column and output column features\n",
    "        vec_assembler = VectorAssembler(inputCols=['latitude', 'longitude'], outputCol=\"features\")\n",
    "        #dataframe with latitude and longitude\n",
    "        lat_long_df = base_data_df.select('latitude', 'longitude')\n",
    "        #vector transform for latitude and longitude\n",
    "        vec_transform = vec_assembler.transform(lat_long_df)\n",
    "        \n",
    "        return vec_transform\n",
    "    \n",
    "    def generate_model(self, base_data_df):\n",
    "        \"\"\"\n",
    "        This function generates the model for the datasets which will be used for the daily prediction\n",
    "            Arguments:\n",
    "                base_data_df: base dataset with all the days\n",
    "            Returns:\n",
    "                model: KMeans cluster model with 100 clusters\n",
    "        \"\"\"\n",
    "        #join all the arrays into a single dataset\n",
    "        main_base_df = reduce(DataFrame.unionAll, base_data_df)\n",
    "        #clean the data\n",
    "        main_base_df = mlbd.clean_data(main_base_df)\n",
    "        #select the required data\n",
    "        model_df = main_base_df.select('latitude', 'longitude').distinct()\n",
    "        #split data into training and testing data\n",
    "        train_data, test_data = mlbd.split_data_to_train_and_test(model_df)\n",
    "        #create a vector transform for the input data [lat, long]\n",
    "        vec_transform = self.createVectorTransform(train_data)\n",
    "        #create a KMeans object with 100 clusters and 10 max iteration\n",
    "        kmeans = KMeans(k=100, maxIter=10, seed=1)\n",
    "        \n",
    "        #train the model with the dataset\n",
    "        model = kmeans.fit(vec_transform)\n",
    "\n",
    "        return model, test_data\n",
    "\n",
    "    def compute_model_accuracy(self, model, test_data):\n",
    "        \"\"\"\n",
    "        This function computes the model accuracy\n",
    "            Arguments:\n",
    "                model: the model generated\n",
    "                test_data: sample test data\n",
    "            Returns:\n",
    "                model_accuracy: float\n",
    "        \"\"\"\n",
    "        #compute predictions using the test data\n",
    "        testdf = mlbd.compute_predictions(model, test_data)\n",
    "        #create a vector transform for the test data [lat, long]\n",
    "        vecTransform = mlbd.createVectorTransform(testdf)\n",
    "        #generate a test prediction\n",
    "        prediction = model.transform(vecTransform)\n",
    "\n",
    "        #evaluate the accuracy of the prediction\n",
    "        evaluator = ClusteringEvaluator()\n",
    "        #set prediction column\n",
    "        evaluator.setPredictionCol(\"prediction\")\n",
    "        #compute the accuracy percentage\n",
    "        model_accuracy = evaluator.evaluate(prediction)*100\n",
    "        \n",
    "        return model_accuracy\n",
    "    \n",
    "    def clean_and_format_dataframe(self, df):\n",
    "        \"\"\"\n",
    "        This function cleans and format input dataframe\n",
    "            Arguments:\n",
    "                df: input dataframe\n",
    "            Returns:\n",
    "                df: clean and formatted dataframe\n",
    "        \"\"\"\n",
    "        # clean the required each day data by extracting the sensor information\n",
    "        df = self.clean_data(df)\n",
    "        # extract location information with the corresponding P1 and P2\n",
    "        df = df.select('latitude', 'longitude', 'country', 'P1', 'P2').withColumn('P1', round(col('P1'), 4).cast(FloatType()))\\\n",
    "        .withColumn('P2', round(col('P2'), 4).cast(FloatType()))\\\n",
    "        .groupby(col('latitude'), col('longitude'), col('country')).avg('P1', 'P2').withColumnRenamed('avg(P1)', 'P1').withColumnRenamed('avg(P2)', 'P2')\n",
    "        # generate prediction\n",
    "        df = self.compute_predictions(model, df)\n",
    "        # generate AQI for each day of data\n",
    "        df = self.generate_AQI(df)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def compute_predictions(self, model, dataframe):    \n",
    "        \"\"\"\n",
    "        This function uses the generated model to predict cluster for each day\n",
    "            Arguments: \n",
    "                model: KMeans model with 100 clusters\n",
    "                dataframe: the current dataframe for the day\n",
    "            Returns:\n",
    "                dataframe: dataframe with the prediction\n",
    "        \"\"\"\n",
    "        #create a vector assembler with latitude and longitude column and output column features\n",
    "        vecTransform = self.createVectorTransform(dataframe)\n",
    "        #generate a prediction\n",
    "        prediction = model.transform(vecTransform)\n",
    "        #remove the features column\n",
    "        prediction = prediction.drop('features')\n",
    "        # join the cluster prediction to the dataset\n",
    "        dataframe = dataframe.join(prediction, ['latitude', 'longitude'], 'inner')\n",
    "\n",
    "        return dataframe\n",
    "\n",
    "    def generate_cluster(self, model, dataframe):\n",
    "        \"\"\"\n",
    "        This function generates the cluster center for each of the days\n",
    "            Arguments: \n",
    "                model: KMeans model with 100 clusters\n",
    "                dataframe: the current dataframe for the day\n",
    "            Returns:\n",
    "                dataframe: dataframe with the prediction\n",
    "        \"\"\"\n",
    "        #create a dataframe for the cluster centers \n",
    "        cluster_centers_df = pd.DataFrame(model.clusterCenters(), columns=['longitude', 'latitude'])\n",
    "        #extract the index of for each of the cluster centers\n",
    "        cluster_centers_df['cluster_index'] = cluster_centers_df.index.map(lambda idx: format(idx))\n",
    "        #create a spark dataframe for the dataframe\n",
    "        cluster_centers_df = spark.createDataFrame(cluster_centers_df)\n",
    "        #create a cluster of longitude and latitude\n",
    "        cluster_centers_df = cluster_centers_df.withColumn('cluster_center', concat(array(col('longitude'), col('latitude')))).drop('longitude', 'latitude')\n",
    "        #add the dataframe cluster center to the dataframe\n",
    "        result = dataframe.join(cluster_centers_df, dataframe.prediction == cluster_centers_df.cluster_index, 'inner')\n",
    "        #remove unused columns\n",
    "        result = result.drop('cluster_index', 'latitude', 'longitude', 'country')\n",
    "\n",
    "        return result\n",
    "\n",
    "    def split_data_to_train_and_test(self, dataFrame, training_percentage=0.8):\n",
    "        \"\"\"\n",
    "        This function splits the input data into training and test data based on the percentage of train\n",
    "            Arguments:\n",
    "                dataFrame: DataFrame\n",
    "                train_length_percentage: the percentage of the training (over hundred)\n",
    "            Returns:\n",
    "                tuple: sizeOfTheDataFrame(columns), X_training_dataset, Y_training_dataset, X_test_dataset, Y_test_dataset\n",
    "        \"\"\"\n",
    "        #get test percentage from the training percentage. training percentage default is 80% and test is 20%\n",
    "        test_percentage = 1 - training_percentage\n",
    "        train_data = dataFrame.sample(fraction=training_percentage, seed=1)\n",
    "        test_data = dataFrame.sample(fraction=test_percentage, seed=1)\n",
    "\n",
    "        return train_data, test_data\n",
    "\n",
    "    def merge_multiple_days_AQI(self, model, dataframes):\n",
    "        \"\"\"\n",
    "        This function merges multiple days together\n",
    "            Arguments: \n",
    "                dfs: the dataframes array\n",
    "            Returns:\n",
    "                result: the merged dataframe for the multiple days\n",
    "        \"\"\"\n",
    "        result = []\n",
    "\n",
    "        for i in range(0, len(dataframes)):\n",
    "            curr = dataframes[i]\n",
    "\n",
    "            #generate cluster center for each day\n",
    "            curr = self.generate_cluster(model, curr)\n",
    "            #reformat each AQI\n",
    "            curr = curr.withColumnRenamed('AQI', f'AQI_{i+1}').withColumn(f'AQI_{i+1}', col(f'AQI_{i+1}').cast(IntegerType()))\n",
    "            #group the AQIs by prediction and cluster_center\n",
    "            curr = curr.groupBy('prediction', 'cluster_center').avg().drop('avg(prediction)').withColumnRenamed(f'avg(AQI_{i+1})', f'AQI_{i+1}')\n",
    "            #get the ceil of the average\n",
    "            curr = curr.withColumn(f'AQI_{i+1}', ceil(f'AQI_{i+1}'))\n",
    "\n",
    "            if result:\n",
    "                result = result.join(curr, ['prediction', 'cluster_center' ], 'leftouter')\n",
    "            else:\n",
    "                result = curr\n",
    "                \n",
    "        return result\n",
    "\n",
    "\n",
    "    def generate_AQI(self, df):\n",
    "        \"\"\"\n",
    "        This function generates the Air Quality Indeces of the each of the prediction\n",
    "            Arguments:\n",
    "                df: the dataframe for the day\n",
    "            Returns:\n",
    "                df: the dataframe with AQI, and AQI_Range\n",
    "        \"\"\"\n",
    "        #compute P1 AQI \n",
    "        df = df.withColumn('P1_AQI', \\\n",
    "                         when((df.P1 >= 0) & (df.P1 < 17), lit(1))\\\n",
    "                         .when((df.P1 >= 17) & (df.P1 < 34), lit(2))\\\n",
    "                         .when((df.P1 >= 34) & (df.P1 < 51), lit(3))\\\n",
    "                         .when((df.P1 >= 51) & (df.P1 < 59), lit(4))\\\n",
    "                         .when((df.P1 >= 59) & (df.P1 < 67), lit(5))\\\n",
    "                         .when((df.P1 >= 67) & (df.P1 < 76), lit(6))\\\n",
    "                         .when((df.P1 >= 76) & (df.P1 < 84), lit(7))\\\n",
    "                         .when((df.P1 >= 84) & (df.P1 < 92), lit(8))\\\n",
    "                         .when((df.P1 >= 92) & (df.P1 <= 100), lit(9))\\\n",
    "                         .when((df.P1 > 100), lit(10))\\\n",
    "                        )\n",
    "        #compute P1 AQI \n",
    "        df = df.withColumn('P2_AQI', \\\n",
    "                             when((df.P2 >= 0) & (df.P2 < 12), lit(1))\\\n",
    "                             .when((df.P2 >= 12) & (df.P2 < 24), lit(2))\\\n",
    "                             .when((df.P2 >= 24) & (df.P2 < 36), lit(3))\\\n",
    "                             .when((df.P2 >= 36) & (df.P2 < 42), lit(4))\\\n",
    "                             .when((df.P2 >= 42) & (df.P2 < 48), lit(5))\\\n",
    "                             .when((df.P2 >= 48) & (df.P2 < 54), lit(6))\\\n",
    "                             .when((df.P2 >= 54) & (df.P2 < 59), lit(7))\\\n",
    "                             .when((df.P2 >= 59) & (df.P2 < 65), lit(8))\\\n",
    "                             .when((df.P2 >= 65) & (df.P2 <= 70), lit(9))\\\n",
    "                             .when((df.P2 > 70), lit(10))\\\n",
    "                            )\n",
    "        #compute AQI using the max AQI\n",
    "        df = df.withColumn('AQI', when((df.P1_AQI < df.P2_AQI), lit(df.P2_AQI)).otherwise(lit(df.P1_AQI)))\n",
    "        #get the range of the AQI\n",
    "        df = df.withColumn('AQI_Range', \\\n",
    "                             when((df.AQI >= 1) & (df.AQI <= 3), lit('Low'))\\\n",
    "                             .when((df.AQI >= 4) & (df.AQI <= 6), lit('Medium'))\\\n",
    "                             .when((df.AQI >= 7) & (df.AQI <= 9), lit('High'))\\\n",
    "                             .when((df.AQI == 10), lit('Very High'))\\\n",
    "                            )\n",
    "        #remove unused columns\n",
    "        df = df.drop('P1', 'P2', 'P1_AQI', 'P2_AQI', 'AQI_Range')\n",
    "\n",
    "        return df\n",
    "\n",
    "    def compute_cluster_streak(self, dataframe):\n",
    "        \"\"\"\n",
    "        This function computes the daily streaks of each of the clusters in the dataset\n",
    "            Arguments:\n",
    "                dataframe: the dataframe with the multiple days AQI\n",
    "            Returns:\n",
    "                df: the dataframe with AQI, and AQI_Range\n",
    "        \"\"\"\n",
    "        newRow = []\n",
    "        for row in dataframe.collect():\n",
    "            tempCol = []\n",
    "            x = 1\n",
    "            for col in range(0, len(row)):\n",
    "                if col > 1:\n",
    "                    if(row[col] > 3):\n",
    "                        x = 0\n",
    "                        tempCol.append(x)\n",
    "                    else:\n",
    "                        if x == 0:\n",
    "                            x = 1\n",
    "                        tempCol.append(x)\n",
    "                        x+=1\n",
    "                else:\n",
    "                    tempCol.append(row[col])\n",
    "            newRow.append(tempCol)\n",
    "        return newRow\n",
    "\n",
    "    def compute_general_streak(self, dataframe):\n",
    "        \"\"\"\n",
    "        This function computes the daily streaks of each of the days in the dataset\n",
    "            Arguments:\n",
    "                dataframe: the dataframe with the multiple days AQI\n",
    "            Returns:\n",
    "                df: the dataframe with AQI, and AQI_Range\n",
    "        \"\"\"\n",
    "        newRow = []\n",
    "        for row in dataframe.collect():\n",
    "            newRow.append(np.max(row[2:]))\n",
    "        return newRow\n",
    "\n",
    "    \n",
    "    def process_multiple_days(self, base_df, no_of_days):\n",
    "        \"\"\"\n",
    "        This function processes the multiple days based on their longitude and latitude\n",
    "            Arguments:\n",
    "                dataframe: the dataframe for the day\n",
    "            Returns:\n",
    "                df: sorted dataframe based on the improvement over the previous 24 hours\n",
    "        \"\"\"\n",
    "        processed_df = []\n",
    "        #default to the numner of days in the base_df if no of days is not specified\n",
    "        days_to_process = no_of_days if no_of_days else len(base_df)\n",
    "        print(f'cleaning and generating Air Quality Index data for {days_to_process} days')\n",
    "        for idx in range(0, len(base_df[0:days_to_process])):\n",
    "            df = base_df[idx]\n",
    "            #clean the data\n",
    "            df = self.clean_data(df)\n",
    "            #extract required fields for computation\n",
    "            df = df.select('longitude', 'latitude', 'P1', 'P2').withColumn('P1', col('P1').cast(FloatType()))\\\n",
    "                .withColumn('P2', col('P2').cast(FloatType()))\n",
    "            #calculating the average\n",
    "            df = df.groupby(col('longitude'), col('latitude')).avg('P1', 'P2')\\\n",
    "                .withColumnRenamed('avg(P1)', 'P1').withColumnRenamed('avg(P2)', 'P2')\n",
    "            #generate AQI for the dataframe\n",
    "            df = self.generate_AQI(df)\n",
    "            processed_df.append(df)\n",
    "        return processed_df\n",
    "     \n",
    "    def get_marker_color(self, arr, plot_type='histogram'):\n",
    "        color = ''\n",
    "        if plot_type == 'histogram':\n",
    "            size = np.max(arr[2:])\n",
    "            if size < 4:\n",
    "                color = 'green'\n",
    "            elif size >= 4 and size < 7:\n",
    "                color = 'orange'\n",
    "            elif size >= 7:\n",
    "                color = 'red'\n",
    "        elif plot_type == 'continents':\n",
    "            size = np.max(arr[2:])\n",
    "            if size < 5:\n",
    "                color = 'red'\n",
    "            elif size >= 5 and size <= 15:\n",
    "                color = 'orange'\n",
    "            elif size == 16:\n",
    "                color = 'green'\n",
    "        else:\n",
    "            size = np.max(arr[2:])\n",
    "            if size == 16:\n",
    "                color = 'green'\n",
    "            elif size >= 5 and size <= 15:\n",
    "                color = 'orange'\n",
    "            elif size >= 0 and size < 5:\n",
    "                color = 'red'\n",
    "        return color\n",
    "\n",
    "    def plotMap(self, df):\n",
    "        df = df[['cluster_center', 'Improvement']]\n",
    "        # marker_colors = [\"darkgreen\", \"green\",\"lightblue\",  \"orange\", \"red\"]\n",
    "        marker_colors = [\"orange\",\"lightblue\",  \"green\", \"red\"]\n",
    "        colormap = branca.colormap.LinearColormap(\n",
    "            vmin=1,\n",
    "            vmax=10,\n",
    "            colors=marker_colors,\n",
    "            caption=\"Air Quality Index\")\n",
    "        m = folium.Map(zoom_start=4, location=df.collect()[0][0])\n",
    "\n",
    "        for row in df.collect():\n",
    "            location = row[0]\n",
    "            folium.Marker(location, icon=folium.Icon(color=marker_colors[row[1]])).add_to(m)   \n",
    "\n",
    "        colormap.add_to(m)\n",
    "\n",
    "        return m\n",
    "\n",
    "    def plotMapStreak(self, streakdf, plot_type, show_popup=True):\n",
    "        colormap = branca.colormap.LinearColormap(\n",
    "            vmin=1,\n",
    "            vmax=10,\n",
    "            colors=[\"darkgreen\", \"green\",\"lightblue\",  \"orange\", \"red\"],\n",
    "            caption=\"Air Quality Index\")\n",
    "\n",
    "        tile = 'Cartodb Positron' if plot_type in ['streak', 'histogram'] else 'Stamen Terrain'\n",
    "        # create a map and instantiate it to the first cluster center\n",
    "        m = folium.Map(zoom_start=3.5, location=streakdf.collect()[0][1], tiles=tile, no_wrap=True)\n",
    "\n",
    "        #create a marker for each cluster\n",
    "        for row in streakdf.collect():\n",
    "            title, location = row[0:2]\n",
    "            source = pd.DataFrame(\n",
    "                {\n",
    "                    'Days': streakdf.columns[2:],\n",
    "                    # 'AQI': map(lambda x: 'null' if x == 0 else x, row[2:])\n",
    "                    'AQI': row[2:]\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # create an altair chart\n",
    "            if plot_type == 'streak':\n",
    "                chart = alt.Chart(source, title=f'Cluster_{title}').mark_bar().encode(x='Days', y='AQI')\n",
    "            else:\n",
    "                chart = alt.Chart(source, title=f'Cluster_{title}').mark_bar().encode(alt.X('AQI'), y='count()')\n",
    "\n",
    "            if show_popup:\n",
    "                folium.Marker(location, icon=folium.Icon(icon='spinner', prefix='fa', color=mlbd.get_marker_color(row, plot_type)), \n",
    "                              popup=folium.Popup(max_width=400).add_child(folium.VegaLite(chart.to_json()))\n",
    "                            ).add_to(m)   \n",
    "            else:\n",
    "                folium.Marker(location, icon=folium.Icon(icon='spinner', prefix='fa', color=mlbd.get_marker_color(row, plot_type))).add_to(m)   \n",
    "\n",
    "        colormap.add_to(m)\n",
    "\n",
    "        return m\n",
    "    \n",
    "    def task1(self, dataframe):\n",
    "        \"\"\"\n",
    "        This function processes the top 10 AQI for 2 days based on their improvement over the previous 24 hours\n",
    "            Arguments:\n",
    "                dataframe: the dataframe for the day\n",
    "            Returns:\n",
    "                df: sorted dataframe based on the improvement over the previous 24 hours\n",
    "        \"\"\"\n",
    "        processed_df = []\n",
    "        for idx in range(0, len(dataframe[0:2])):\n",
    "            df = dataframe[idx]\n",
    "            #clean the data\n",
    "            print(f'cleaning and generating Air Quality Index for day {idx+1}')\n",
    "            df = mlbd.clean_data(df)\n",
    "            #extract the required column and compute the average\n",
    "            df = df.select('timestamp', 'country', 'P1', 'P2').withColumn('P1', col('P1').cast(FloatType()))\\\n",
    "            .withColumn('P2', col('P2').cast(FloatType())).groupby(col('country')).avg('P1', 'P2').withColumnRenamed('avg(P1)', 'P1').withColumnRenamed('avg(P2)', 'P2')\n",
    "            #generate AQI for the dataframe\n",
    "            df = self.generate_AQI(df)\n",
    "            \n",
    "            #add dataframe to list of processed dataframe\n",
    "            processed_df.append(df)\n",
    "\n",
    "        for idx in range(0,2, 2):\n",
    "            #get the current and the next dataframe\n",
    "            curr_df = processed_df[idx]\n",
    "            next_df = processed_df[idx+1]\n",
    "            \n",
    "            #reformat AQI dataframe with their respective dates\n",
    "            df = curr_df.select('country', 'AQI').withColumnRenamed('AQI', json_dates[idx]).join(next_df.select('country', 'AQI').withColumnRenamed('AQI', json_dates[idx+1]), ['country'], 'inner')\n",
    "            #get the difference between day1 AQI and day2 AQI\n",
    "            print('computing Air Quality Index improvement information...')\n",
    "            df = df.withColumn(f'Improvement', lit(col(json_dates[idx]) - col(json_dates[idx+1])))\n",
    "            print('computing country names')\n",
    "            country_names = self.compute_country_names(df)\n",
    "            df = df.join(country_names, ['country'], 'right')\n",
    "            df = df.select('country', 'country_name', col(json_dates[idx]), col(json_dates[idx+1]), 'Improvement')\n",
    "            print('ranking top 10 AQI improvement over the previous 24 hours')\n",
    "            df = df.sort(col('Improvement').desc())\n",
    "            return df\n",
    "\n",
    "\n",
    "    def task2(self, base_df, no_of_days):\n",
    "        #combine all the datasets to a single dataframe to create a model\n",
    "        arrs = self.process_multiple_days(base_df, no_of_days)\n",
    "        for idx in range(0,2, 2):\n",
    "            curr_df = arrs[idx]\n",
    "            if idx < len(arrs)-1:\n",
    "                next_df = arrs[idx+1]\n",
    "\n",
    "            df = curr_df.select('latitude', 'longitude', 'AQI').withColumnRenamed('AQI', json_dates[idx])\\\n",
    "                .join(next_df.select('latitude', 'longitude', 'AQI').withColumnRenamed('AQI', json_dates[idx+1]), ['latitude', 'longitude'], 'inner')\n",
    "            #get the difference between day1 AQI and day2 AQI\n",
    "            # df.sort(col('latitude'), col('longitude')).show()\n",
    "            df = self.compute_predictions(model, df)\n",
    "            \n",
    "            # df = clean_and_format_dataframe(df)\n",
    "            df = self.generate_cluster(model, df)\n",
    "            df = df.groupby(col('cluster_center'), col('prediction')).avg(json_dates[idx], json_dates[idx+1])\\\n",
    "                .withColumnRenamed(f'avg({json_dates[idx]})', json_dates[idx]).withColumnRenamed(f'avg({json_dates[idx+1]})', json_dates[idx+1])\\\n",
    "                .withColumn(json_dates[idx], round(col(json_dates[idx])).cast(IntegerType()))\\\n",
    "                .withColumn(json_dates[idx+1], round(col(json_dates[idx+1])).cast(IntegerType()))\n",
    "            # print('computing Air Quality Index improvement information...')\n",
    "            df = df.withColumn(f'Improvement', lit(col(json_dates[idx]) - col(json_dates[idx+1])))\n",
    "            df = df.sort(col('Improvement').desc())\n",
    "\n",
    "            return df\n",
    "\n",
    "    def task3(self, base_df, model):\n",
    "        arrs = self.process_multiple_days(base_df, len(base_df))\n",
    "        arrs = [self.compute_predictions(model, df) for df in arrs]\n",
    "        newdf = self.merge_multiple_days_AQI(model, arrs)\n",
    "        newdf = newdf.fillna(0)\n",
    "        print(f'computing streak for {len(arrs)} days')\n",
    "        streakdf = self.compute_cluster_streak(newdf)\n",
    "        dsstreakdf = self.compute_general_streak(newdf)\n",
    "        streakdf = pd.DataFrame(streakdf)\n",
    "        streakdf.columns = newdf.columns\n",
    "        streakdf = spark.createDataFrame(streakdf)\n",
    "\n",
    "        source = pd.DataFrame({'AQI': dsstreakdf})\n",
    "        plt = sns.histplot(source, x='AQI', bins=8)\n",
    "        \n",
    "        for i in range(0, len(streakdf.columns[2:])):\n",
    "            streakdf = streakdf.withColumnRenamed(f'AQI_{i+1}', json_dates[i])\n",
    "\n",
    "        return streakdf, newdf, plt\n",
    "\n",
    "mlbd = MLBD()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb20c73-88f5-407d-afc4-44f89a04317e",
   "metadata": {},
   "source": [
    "## PREPARING AND EXTRACTING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "f94adbfd-a902-4b51-9274-d5b506ae68ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [173]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m      2\u001b[0m json_dates \u001b[38;5;241m=\u001b[39m mlbd\u001b[38;5;241m.\u001b[39mextract_computation_dates()\n\u001b[0;32m----> 3\u001b[0m base_df \u001b[38;5;241m=\u001b[39m \u001b[43mmlbd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_all_jsons\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal computation time (HH:MM:SS)\u001b[39m\u001b[38;5;124m'\u001b[39m, datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m start_time)\n",
      "Input \u001b[0;32mIn [172]\u001b[0m, in \u001b[0;36mMLBD.read_all_jsons\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03mThis functions reads all the datasets downloaded into datasets folder\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m        base_data_df: list of dataframes for each day\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# add all files in datasets directory to spark\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatasets\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#get all the data in the dataset directory as json\u001b[39;00m\n\u001b[1;32m     26\u001b[0m jsons \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets/*.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pyspark/context.py:1248\u001b[0m, in \u001b[0;36mSparkContext.addFile\u001b[0;34m(self, path, recursive)\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maddFile\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m, recursive: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;124;03m    Add a file to be downloaded with this Spark job on every node.\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;124;03m    The `path` passed can be either a local file, a file in HDFS\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;124;03m    [100, 200, 300, 400]\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1248\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m()\u001b[38;5;241m.\u001b[39maddFile(path, recursive)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sc'"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "json_dates = mlbd.extract_computation_dates()\n",
    "base_df = mlbd.read_all_jsons()\n",
    "print(f'Total computation time (HH:MM:SS)', datetime.datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0d6ba1-bfc0-4cc8-a273-47546358dc44",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MODEL FOR THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37af228-9245-4bec-bd7e-80ec76c2ea77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "model, test_data = mlbd.generate_model(base_df)\n",
    "print(f'Total computation time (HH:MM:SS)', datetime.datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb7a13f-8079-44db-9eb6-378f5c69afaf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TASK 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c39714-5f6a-4cbb-afbc-775bd52df740",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Top 10 country in terms of average AQI over the previous 24 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0b8226-6a7c-41b1-9c0f-bfd6811495c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "df = mlbd.task1(base_df)\n",
    "df.where(col('Improvement')>0).show(10, truncate=False)\n",
    "print(f'Total computation time (HH:MM:SS)', datetime.datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c5a790-5c9d-430a-9e0b-9b47fc4abd4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cad94d6-0bed-450a-a03d-5dad01dc5e36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "# process only 2 days\n",
    "df = mlbd.task2(base_df, 2)\n",
    "# selecting the top 50 data\n",
    "df = spark.createDataFrame(df.head(50))\n",
    "df.show(50, truncate=False)\n",
    "m = mlbd.plotMap(df)\n",
    "print(f'Total computation time (HH:MM:SS)', datetime.datetime.now() - start_time, '\\n')\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8842e05-06d6-4cee-8390-50513e565f6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c67eb848-ec77-4700-869f-020fb6514f9d",
   "metadata": {},
   "source": [
    "### Testing Accuracy of the Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965fbbcc-be89-4170-aaaf-5f03c2718ec9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "model_accuracy = mlbd.compute_model_accuracy(model, test_data)\n",
    "print(f'Model Accuracy %.2f%%' % (model_accuracy))\n",
    "print(f'Total computation time (HH:MM:SS)', datetime.datetime.now() - start_time, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48238fcc-55aa-41cd-8ed5-afb1c4c911ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11036275-6a44-41e0-a55d-6888f8a7e5df",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ad5a8b-0f22-4141-8093-b0583ed08c30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "streakdf, newdf, streakhist = mlbd.task3(base_df, model)\n",
    "print(f'Total computation time (HH:MM:SS)', datetime.datetime.now() - start_time, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddfd039-cff6-46a7-ab41-29f674554d23",
   "metadata": {},
   "source": [
    "### Testing Accuracy of the Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eda7fd7-f715-4dcf-b184-bb98fbf3b4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "model_accuracy = mlbd.compute_model_accuracy(model, test_data)\n",
    "print(f'Model Accuracy %.2f%%' % (model_accuracy))\n",
    "print(f'Total computation time (HH:MM:SS)', datetime.datetime.now() - start_time, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f853075-c8de-4fc5-aaef-0a8c12bbbd6d",
   "metadata": {},
   "source": [
    "## Plotting Results On the Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe81b2f1-060b-410f-82cd-11d3bde0d1bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clustermap = mlbd.plotMapStreak(newdf, plot_type='histogram')\n",
    "clustermap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1cc8e5-6b8b-4a70-8fc5-77e0f0ff6568",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "streakmap = mlbd.plotMapStreak(streakdf, 'streak')\n",
    "streakmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaf7d6e-27c3-40b4-9d7a-c309671e25ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "continentmap = mlbd.plotMapStreak(streakdf, 'continents', show_popup=False)\n",
    "continentmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c642ccbc-c791-4092-bbfc-cf2037ce41c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add .\n",
    "!git commit -m \"update code implementation\"\n",
    "!git push"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b75cf6cb7ff0655e157d309e117e70ffefae2076bef6ebc3b7409f43a5f1c70f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
